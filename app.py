import streamlit as st
from streamlit_echarts import st_pyecharts
import requests
from bs4 import BeautifulSoup
import jieba
from collections import Counter
import re
import warnings
warnings.filterwarnings('ignore')

# -------------------------- å·¥å…·å‡½æ•° --------------------------
def crawl_url_text(url):
    """æŠ“å–URLçš„æ–‡æœ¬å†…å®¹"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.encoding = response.apparent_encoding or 'utf-8'
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ç§»é™¤scriptã€styleã€navã€footerç­‰éæ­£æ–‡æ ‡ç­¾
        for tag in soup(['script', 'style', 'nav', 'footer', 'header', 'iframe', 'aside']):
            tag.decompose()
        
        # æå–æ­£æ–‡ï¼ˆä¼˜å…ˆå–articleã€div[class*="content"]ã€pæ ‡ç­¾ï¼‰
        article = soup.find('article')
        if article:
            text = article.get_text(strip=True, separator='\n')
        else:
            content_div = soup.find('div', class_=re.compile(r'content|article|main', re.I))
            if content_div:
                text = content_div.get_text(strip=True, separator='\n')
            else:
                # æå–æ‰€æœ‰pæ ‡ç­¾æ–‡æœ¬
                p_tags = soup.find_all('p')
                text = '\n'.join([p.get_text(strip=True) for p in p_tags if p.get_text(strip=True)])
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        if not text or len(text) < 50:
            st.error("æœªèƒ½æå–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼ˆå¯èƒ½æ˜¯åçˆ¬æˆ–é¡µé¢ç»“æ„ä¸æ”¯æŒï¼‰ï¼")
            return ""
        
        return text
    except Exception as e:
        st.error(f"URLæŠ“å–å¤±è´¥ï¼š{str(e)}")
        return ""

def clean_and_cut_text(text):
    """æ¸…æ´—æ–‡æœ¬å¹¶åˆ†è¯"""
    # 1. æ¸…æ´—ï¼šåªä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 2. åˆ†è¯
    words = jieba.lcut(text)
    
    # 3. è¿‡æ»¤åœç”¨è¯å’Œæ— æ•ˆè¯æ±‡ï¼ˆå•å­—ã€ç©ºç™½ï¼‰
    stop_words = set([
        'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'æˆ‘', 'åˆ°', 'æ¥', 'å»', 'ä¸Š', 'ä¸‹', 'å¤§', 'å°',
        'å¤š', 'å°‘', 'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å', 'ç™¾', 'åƒ', 'ä¸‡', 'äº¿',
        'è¿™', 'é‚£', 'å“ª', 'æ­¤', 'å½¼', 'å…¶', 'å®ƒ', 'ä»–', 'å¥¹', 'ä½ ', 'æˆ‘', 'ä»–', 'æˆ‘ä»¬', 'ä½ ä»¬', 'ä»–ä»¬',
        'è¿™é‡Œ', 'é‚£é‡Œ', 'å“ªé‡Œ', 'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€ä¹ˆ', 'å› ä¸º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'è€Œä¸”', 'è™½ç„¶', 'å¦‚æœ',
        'å¯¹äº', 'å…³äº', 'ä¸€å®š', 'å¯èƒ½', 'å¯ä»¥', 'åº”è¯¥', 'éœ€è¦', 'ä¼š', 'è¦', 'æ²¡', 'æ²¡æœ‰', 'è¿˜', 'ä¹Ÿ', 'éƒ½',
        'åª', 'åª', 'åˆ', 'å†', 'æ›´', 'æœ€', 'å¾ˆ', 'éå¸¸', 'ç‰¹åˆ«', 'æ¯”è¾ƒ', 'ç¨å¾®', 'å‡ ä¹', 'å·®ä¸å¤š',
        'ç€', 'è¿‡', 'è¿‡', 'å‘¢', 'å—', 'å§', 'å•Š', 'å‘€', 'å“¦', 'å—¯', 'å“ˆ', 'å“¼', 'å‘µ',
        'http', 'https', 'com', 'www', 'html', 'php', 'jsp', 'asp', 'css', 'js', 'img', 'src', 'href'
    ])
    
    valid_words = [
        word for word in words 
        if len(word) > 1  # è¿‡æ»¤å•å­—
        and word not in stop_words  # è¿‡æ»¤åœç”¨è¯
        and not word.isdigit()  # è¿‡æ»¤çº¯æ•°å­—
        and len(word.strip()) > 0  # è¿‡æ»¤ç©ºç™½
    ]
    
    return valid_words

def generate_chart(chart_type, top20_words):
    """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
    words = [item[0] for item in top20_words]
    counts = [item[1] for item in top20_words]
    
    from pyecharts import options as opts
    from pyecharts.charts import Bar, Line, WordCloud, Pie, Radar, Scatter
    
    if chart_type == "è¯äº‘":
        chart = (
            WordCloud()
            .add("", list(zip(words, counts)), word_size_range=[20, 100])
            .set_global_opts(title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - è¯äº‘"))
        )
    elif chart_type == "è¯é¢‘æŸ±çŠ¶å›¾":
        chart = (
            Bar()
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", counts)
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - æŸ±çŠ¶å›¾"),
                xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=-45)),
                legend_opts=opts.LegendOpts(is_show=False)
            )
        )
    elif chart_type == "è¯é¢‘æŠ˜çº¿å›¾":
        chart = (
            Line()
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", counts, markpoint_opts=opts.MarkPointOpts(data=[opts.MarkPointItem(type_="max")]))
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - æŠ˜çº¿å›¾"),
                xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=-45)),
                legend_opts=opts.LegendOpts(is_show=False)
            )
        )
    elif chart_type == "è¯é¢‘é¥¼å›¾":
        chart = (
            Pie()
            .add("", list(zip(words, counts)))
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - é¥¼å›¾"),
                legend_opts=opts.LegendOpts(orient="vertical", pos_top="15%", pos_left="2%")
            )
            .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c}"))
        )
    elif chart_type == "è¯é¢‘ç¯å½¢å›¾":
        chart = (
            Pie()
            .add("", list(zip(words, counts)) , radius=["40%", "70%"])
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - ç¯å½¢å›¾"),
                legend_opts=opts.LegendOpts(orient="vertical", pos_top="15%", pos_left="2%")
            )
            .set_series_opts(label_opts=opts.LabelOpts(formatter="{b}: {c}"))
        )
    elif chart_type == "è¯é¢‘é›·è¾¾å›¾":
        # é›·è¾¾å›¾éœ€è¦æ„é€ ç»´åº¦æ•°æ®
        radar_data = [{"name": words[i], "value": [counts[i]]} for i in range(len(words))]
        schema = [{"name": "è¯é¢‘", "max": max(counts), "min": min(counts)}]
        
        chart = (
            Radar()
            .add_schema(schema)
            .add("è¯é¢‘", radar_data)
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - é›·è¾¾å›¾"),
                legend_opts=opts.LegendOpts(is_show=False)
            )
        )
    elif chart_type == "è¯é¢‘æ•£ç‚¹å›¾":
        chart = (
            Scatter()
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", counts)
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - æ•£ç‚¹å›¾"),
                xaxis_opts=opts.AxisOpts(axislabel_opts=opts.LabelOpts(rotate=-45)),
                yaxis_opts=opts.AxisOpts(min_=0),
                legend_opts=opts.LegendOpts(is_show=False)
            )
        )
    elif chart_type == "è¯é¢‘æ¡å½¢å›¾":
        chart = (
            Bar()
            .add_xaxis(words)
            .add_yaxis("è¯é¢‘", counts)
            .reversal_axis()  # åè½¬åæ ‡è½´å®ç°æ¡å½¢å›¾
            .set_global_opts(
                title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - æ¡å½¢å›¾"),
                legend_opts=opts.LegendOpts(is_show=False)
            )
        )
    else:  # é»˜è®¤è¯äº‘
        chart = (
            WordCloud()
            .add("", list(zip(words, counts)), word_size_range=[20, 100])
            .set_global_opts(title_opts=opts.TitleOpts(title="è¯é¢‘Top20 - è¯äº‘"))
        )
    return chart

# -------------------------- ä¸»ç¨‹åº --------------------------
def main():
    # ä¾§è¾¹æ è®¾ç½®
    st.sidebar.title("âš™ï¸ é…ç½®é€‰é¡¹")
    chart_type = st.sidebar.selectbox(
        "ğŸ“Š é€‰æ‹©å¯è§†åŒ–å›¾è¡¨ç±»å‹",
        ["è¯äº‘", "è¯é¢‘æŸ±çŠ¶å›¾", "è¯é¢‘æŠ˜çº¿å›¾", "è¯é¢‘é¥¼å›¾", "è¯é¢‘ç¯å½¢å›¾", "è¯é¢‘é›·è¾¾å›¾", "è¯é¢‘æ•£ç‚¹å›¾", "è¯é¢‘æ¡å½¢å›¾"],
        index=0
    )
    min_freq = st.sidebar.number_input(
        "ğŸ” ä½é¢‘è¯è¿‡æ»¤é˜ˆå€¼ï¼ˆæœ€å°è¯é¢‘ï¼‰",
        min_value=1,
        value=2,
        step=1,
        help="è¿‡æ»¤è¯é¢‘å°äºè¯¥å€¼çš„è¯æ±‡ï¼Œä»…ä¿ç•™é«˜é¢‘è¯"
    )

    # ä¸»é¡µé¢ï¼šURLè¾“å…¥ + åˆ†æ
    st.title("ğŸ“ URLæ–‡æœ¬è¯é¢‘åˆ†æå·¥å…·")
    st.divider()
    url = st.text_input(
        "è¯·è¾“å…¥æ–‡ç« URL",
        placeholder="ä¾‹å¦‚ï¼šhttps://www.xxx.com/article.html",
        help="æ”¯æŒå¤§éƒ¨åˆ†æ–°é—»ã€åšå®¢ç±»ç½‘é¡µçš„æ–‡æœ¬æå–"
    )

    # åˆ†ææŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹åˆ†æ", type="primary"):
        if not url:
            st.warning("è¯·è¾“å…¥æœ‰æ•ˆçš„URLï¼")
            return
        
        # 1. æŠ“å–URLæ–‡æœ¬
        with st.spinner("æ­£åœ¨æŠ“å–URLæ–‡æœ¬..."):
            raw_text = crawl_url_text(url)
        if not raw_text:
            return
        
        # 2. æ¸…æ´—åˆ†è¯
        with st.spinner("æ­£åœ¨æ¸…æ´—å¹¶åˆ†è¯..."):
            valid_words = clean_and_cut_text(raw_text)
        if not valid_words:
            st.error("åˆ†è¯åæ— æœ‰æ•ˆè¯æ±‡ï¼ˆå¯èƒ½å…¨æ˜¯åœç”¨è¯/å•å­—ï¼‰ï¼")
            return
        
        # 3. è¯é¢‘ç»Ÿè®¡ + è¿‡æ»¤ä½é¢‘è¯
        word_count = Counter(valid_words)
        filtered_words = {word: cnt for word, cnt in word_count.items() if cnt >= min_freq}
        if not filtered_words:
            st.error(f"è¿‡æ»¤åæ— è¯é¢‘â‰¥{min_freq}çš„è¯æ±‡ï¼è¯·é™ä½é˜ˆå€¼é‡è¯•ã€‚")
            return
        
        # 4. å–å‰20è¯é¢‘
        top20_words = sorted(filtered_words.items(), key=lambda x: x[1], reverse=True)[:20]
        
        # å±•ç¤ºæå–å¹¶æ¸…æ´—åçš„å®Œæ•´æ–‡æœ¬
        st.subheader("ğŸ“œ æå–å¹¶æ¸…æ´—åçš„å®Œæ•´æ–‡æœ¬")
        st.text_area(
            label="å®Œæ•´æ–‡æœ¬å†…å®¹",
            value=raw_text,
            height=300,
            disabled=True
        )
        
        # å±•ç¤ºåˆ†è¯åçš„å®Œæ•´æœ‰æ•ˆè¯æ±‡
        st.subheader("âœ‚ï¸ åˆ†è¯åçš„å®Œæ•´æœ‰æ•ˆè¯æ±‡")
        segmented_full_text = " ".join(valid_words)
        st.text_area(
            label="åˆ†è¯ç»“æœ",
            value=segmented_full_text,
            height=300,
            disabled=True
        )
        
        # å±•ç¤ºå‰20è¯é¢‘ï¼ˆç”¨Streamlitå†…ç½®è¡¨æ ¼ï¼Œæ— éœ€pandasï¼‰
        st.subheader("ğŸ† è¯é¢‘æ’åå‰20è¯æ±‡")
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ŒStreamlitå¯ç›´æ¥å±•ç¤º
        top20_list = [[word, cnt] for word, cnt in top20_words]
        st.dataframe(top20_list, column_config={"0": "è¯æ±‡", "1": "è¯é¢‘"})
        
        # 5. ç”Ÿæˆå¹¶å±•ç¤ºå›¾è¡¨
        st.subheader("ğŸ“ˆ å¯è§†åŒ–å›¾è¡¨")
        chart = generate_chart(chart_type, top20_words)
        st_pyecharts(chart, width="100%")

if __name__ == "__main__":
    main()
